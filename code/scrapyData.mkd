##install
```
$sudo apt-get install libffi-dev python-dev python-lxml
$sudo pip install scrapy==0.24.4

$scrapy -v
```

###使用scrapy
```
$cd /path/to/your/project
$scrap startproject nameofscrapy	#建立一个scrapy项目名为nameofscrapy
```

###scrapy文件结构
```
* nameofscrapy	----项目根目录
  * scrapy.cfg	----项目的配置文件，在这里修改配置
  * nameifscrapy----项目的python模块,在这里修改代码
    * __init__.py	----项目的初始化脚本
    * items.py		----项目中的item文件
    * pipelines.py	----项目中的pipelines文件
    * settings.py 	----项目中的设置文件
    * spiders/		----放置spider代码的目录
```

###定义Item
Item 是保存爬取到的数据的容器；其使用方法和python字典类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。

首先根据需要从weather.sina.com.cn获取到的数据对item进行建模。 我们需要从weather.sina.com.cn中获取当前城市名，后续9天的日期，天气描述和温度等信息。对此，在item中定义相应的字段。编辑 weather 目录中的 items.py 文件(gitscrapy/items.py)。

###编写获取天气数据的爬虫(Spider)
Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。

其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 item 的方法。

为了创建一个Spider，**必须继承 scrapy.Spider 类**， 且定义以下三个属性:

* name: 用于区别Spider。该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。
* start_urls: 包含了Spider在启动时进行爬取的url列表。因此，第一个被获取到的页面将是其中之一。后续的URL则从初始的URL获取到的数据中提取。
* parse() 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。

在编写spider时，用到的selector文档。(gitscrapy/spider/test.py)

[selector](http://doc.scrapy.org/en/0.24/topics/selectors.html)

###test
进入到项目的**scrapy.cfg文件同级目录**下运行命令

`scrapy crawl test -o test.json`

命令的意思是，运行名字为 test 的爬虫（我们在上一步中定义的），然后把结果以json格式保存在test.json文件中

###保存数据 item pipeline
如果我们想自己保存在文件或数据库中，如何操作呢？

这里就要用到 Item Pipeline 了，那么 Item Pipeline 是什么呢？

当Item在Spider中被收集之后，它将会被传递到Item Pipeline中，一些组件会按照一定的顺序执行对Item的处理。

每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。

item pipeline的典型应用有：

* 清理HTML数据
* 验证爬取的数据(检查item包含某些字段)
* 查重(并丢弃)
* 将爬取结果保存到文件或数据库中

每个item pipeline组件都需要调用 `process_item` 方法，这个方法必须返回一个 Item (或任何继承类)对象， 或是抛出 DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理。

我们这里用pipeline把数据转码后保存在 wea.txt 文本中。

`pipelines.py`文件在创建项目时已经自动被创建好了，我们在其中加上保存到文件的代码.(gitscrapy/pipelines.py)

###把ITEM——PIPELINES添加到设置中
写好ITEM_PIPELINES后，还有很重要的一步，就是把 ITEM_PIPELINES 添加到设置文件 settings.py 中。(gitscrapy/settings.py)

```
ITEM_PIPELINES={
    'gitScrapy.pipelines.'
}
```

###运行爬虫
在**项目的scrapy.cfg同级目录下运行**

`scrapy crawl nameofthespider`


